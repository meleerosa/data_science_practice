{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "제조.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meleerosa/data_science_practice/blob/main/%EC%A0%9C%EC%A1%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5VCrodhWnw8"
      },
      "source": [
        "# <0. 필요 패키지 설치하기>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkavAYRYWnw_",
        "outputId": "ea932656-763a-441c-e61b-bf3fee5e5315"
      },
      "source": [
        "!pip install keras==2.1.6\n",
        "!pip install tensorflow-gpu==1.15\n",
        "!pip install matplotlib\n",
        "!pip install -U scikit-learn\n",
        "!pip install pandas"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.1.6\n",
            "  Downloading Keras-2.1.6-py2.py3-none-any.whl (339 kB)\n",
            "\u001b[K     |████████████████████████████████| 339 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (1.4.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.1.6) (1.5.2)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.7.0\n",
            "    Uninstalling keras-2.7.0:\n",
            "      Successfully uninstalled keras-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.7.0 requires keras<2.8,>=2.7.0rc0, but you have keras 2.1.6 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.1.6\n",
            "Collecting tensorflow-gpu==1.15\n",
            "  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5 MB 7.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.37.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 57.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.42.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.17.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.19.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.13.3)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=8c32c6885fa1cefa0f4fadfa3ec44ea1078613199671908b45620f2ad4160668\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.7.0 requires keras<2.8,>=2.7.0rc0, but you have keras 2.1.6 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 1.15.1 which is incompatible.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEYyRI7CWnxD"
      },
      "source": [
        "# <1-1. 필요 라이브러리 불러오기>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsaZ8g8lWnxE",
        "outputId": "52b62aa1-b2b5-464d-c432-e764e62c7600"
      },
      "source": [
        "from __future__ import print_function\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# import function libraries\n",
        "import numpy as np\n",
        "import keras\n",
        "import os, sys ,math, copy\n",
        "import scipy.io as sio\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, Sequential\n",
        "from keras.engine import Layer, InputSpec\n",
        "from keras.optimizers import RMSprop, SGD, Adam\n",
        "from keras import initializers, regularizers, constraints\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, History\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
        "from keras import backend as K\n",
        "from keras.utils import np_utils\n",
        "\n",
        "sys.setrecursionlimit(10000)\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REOlaGjeWnxF"
      },
      "source": [
        "# <1-2. 데이터 불러오기>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpzRISxsWnxG"
      },
      "source": [
        "# 파일 경로 확인!\n",
        "# train.csv 파일은 현재 실행한 CNC_network.ipynb 파일과 같은 폴더에 저장되어야 함\n",
        "# 개별 생산 단위 데이터 파일 경로 설정\n",
        "# CNC Virtual Data set _v2 폴더의 파일 경로 복사하여 path 의 ' '안에 붙여넣기\n",
        "\n",
        "train_sample = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/train.csv\", header=0, encoding='utf-8')\n",
        "path = r'/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2'\n",
        "all_files = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "# count the number of pass/fail items\n",
        "\n",
        "train_sample_np = np.array(train_sample.copy())\n",
        "\n",
        "\n",
        "# load csv file\n",
        "li_df = []\n",
        "for filename in all_files:\n",
        "    df = pd.read_csv(filename, index_col=None, header=0)    \n",
        "    li_df.append(df)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orTyFSf6WnxH"
      },
      "source": [
        "# <2-1. 데이터 종류 확인>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "cQ41Ujh7WnxI",
        "outputId": "f9860ce6-de4f-4833-d13c-0003e32c40ec"
      },
      "source": [
        "train_sample"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>material</th>\n",
              "      <th>feedrate</th>\n",
              "      <th>clamp_pressure</th>\n",
              "      <th>tool_condition</th>\n",
              "      <th>machining_finalized</th>\n",
              "      <th>passed_visual_inspection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>unworn</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>20</td>\n",
              "      <td>4.0</td>\n",
              "      <td>unworn</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>6</td>\n",
              "      <td>3.0</td>\n",
              "      <td>unworn</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>6</td>\n",
              "      <td>2.5</td>\n",
              "      <td>unworn</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>20</td>\n",
              "      <td>3.0</td>\n",
              "      <td>unworn</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>worn</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>20</td>\n",
              "      <td>4.0</td>\n",
              "      <td>worn</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>20</td>\n",
              "      <td>4.0</td>\n",
              "      <td>worn</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>15</td>\n",
              "      <td>4.0</td>\n",
              "      <td>worn</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>12</td>\n",
              "      <td>4.0</td>\n",
              "      <td>worn</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>unworn</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>unworn</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>worn</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>worn</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>6</td>\n",
              "      <td>3.0</td>\n",
              "      <td>worn</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>20</td>\n",
              "      <td>3.0</td>\n",
              "      <td>worn</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>unworn</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>worn</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>15</td>\n",
              "      <td>4.0</td>\n",
              "      <td>worn</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>12</td>\n",
              "      <td>4.0</td>\n",
              "      <td>unworn</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>21</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>unworn</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>22</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>20</td>\n",
              "      <td>3.0</td>\n",
              "      <td>worn</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>worn</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>unworn</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>aluminum</td>\n",
              "      <td>6</td>\n",
              "      <td>2.5</td>\n",
              "      <td>worn</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    No  material  ...  machining_finalized  passed_visual_inspection\n",
              "0    1  aluminum  ...                  yes                       yes\n",
              "1    2  aluminum  ...                  yes                       yes\n",
              "2    3  aluminum  ...                  yes                       yes\n",
              "3    4  aluminum  ...                   no                       NaN\n",
              "4    5  aluminum  ...                   no                       NaN\n",
              "5    6  aluminum  ...                  yes                        no\n",
              "6    7  aluminum  ...                   no                       NaN\n",
              "7    8  aluminum  ...                  yes                        no\n",
              "8    9  aluminum  ...                  yes                        no\n",
              "9   10  aluminum  ...                  yes                        no\n",
              "10  11  aluminum  ...                  yes                       yes\n",
              "11  12  aluminum  ...                  yes                       yes\n",
              "12  13  aluminum  ...                  yes                       yes\n",
              "13  14  aluminum  ...                  yes                       yes\n",
              "14  15  aluminum  ...                  yes                       yes\n",
              "15  16  aluminum  ...                   no                       NaN\n",
              "16  17  aluminum  ...                  yes                       yes\n",
              "17  18  aluminum  ...                  yes                       yes\n",
              "18  19  aluminum  ...                  yes                        no\n",
              "19  20  aluminum  ...                   no                       NaN\n",
              "20  21  aluminum  ...                  yes                        no\n",
              "21  22  aluminum  ...                  yes                       yes\n",
              "22  23  aluminum  ...                   no                       NaN\n",
              "23  24  aluminum  ...                  yes                       yes\n",
              "24  25  aluminum  ...                  yes                       yes\n",
              "\n",
              "[25 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "__mE4qjoWnxJ",
        "outputId": "416dac5e-d7a0-4ec9-a86a-4cf66aec506f"
      },
      "source": [
        "df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X_ActualPosition</th>\n",
              "      <th>X_ActualVelocity</th>\n",
              "      <th>X_ActualAcceleration</th>\n",
              "      <th>X_SetPosition</th>\n",
              "      <th>X_SetVelocity</th>\n",
              "      <th>X_SetAcceleration</th>\n",
              "      <th>X_CurrentFeedback</th>\n",
              "      <th>X_DCBusVoltage</th>\n",
              "      <th>X_OutputCurrent</th>\n",
              "      <th>X_OutputVoltage</th>\n",
              "      <th>X_OutputPower</th>\n",
              "      <th>Y_ActualPosition</th>\n",
              "      <th>Y_ActualVelocity</th>\n",
              "      <th>Y_ActualAcceleration</th>\n",
              "      <th>Y_SetPosition</th>\n",
              "      <th>Y_SetVelocity</th>\n",
              "      <th>Y_SetAcceleration</th>\n",
              "      <th>Y_CurrentFeedback</th>\n",
              "      <th>Y_DCBusVoltage</th>\n",
              "      <th>Y_OutputCurrent</th>\n",
              "      <th>Y_OutputVoltage</th>\n",
              "      <th>Y_OutputPower</th>\n",
              "      <th>Z_ActualPosition</th>\n",
              "      <th>Z_ActualVelocity</th>\n",
              "      <th>Z_ActualAcceleration</th>\n",
              "      <th>Z_SetPosition</th>\n",
              "      <th>Z_SetVelocity</th>\n",
              "      <th>Z_SetAcceleration</th>\n",
              "      <th>Z_CurrentFeedback</th>\n",
              "      <th>Z_DCBusVoltage</th>\n",
              "      <th>Z_OutputCurrent</th>\n",
              "      <th>Z_OutputVoltage</th>\n",
              "      <th>S_ActualPosition</th>\n",
              "      <th>S_ActualVelocity</th>\n",
              "      <th>S_ActualAcceleration</th>\n",
              "      <th>S_SetPosition</th>\n",
              "      <th>S_SetVelocity</th>\n",
              "      <th>S_SetAcceleration</th>\n",
              "      <th>S_CurrentFeedback</th>\n",
              "      <th>S_DCBusVoltage</th>\n",
              "      <th>S_OutputCurrent</th>\n",
              "      <th>S_OutputVoltage</th>\n",
              "      <th>S_OutputPower</th>\n",
              "      <th>S_SystemInertia</th>\n",
              "      <th>M_CURRENT_PROGRAM_NUMBER</th>\n",
              "      <th>M_sequence_number</th>\n",
              "      <th>M_CURRENT_FEEDRATE</th>\n",
              "      <th>Machining_Process</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>176.0</td>\n",
              "      <td>10.0500</td>\n",
              "      <td>57.000</td>\n",
              "      <td>176.0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>4.00</td>\n",
              "      <td>-0.620</td>\n",
              "      <td>0.0194</td>\n",
              "      <td>327</td>\n",
              "      <td>0.709</td>\n",
              "      <td>-1.030000e-06</td>\n",
              "      <td>135.50</td>\n",
              "      <td>4.0125</td>\n",
              "      <td>10.250</td>\n",
              "      <td>135.50</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.5610</td>\n",
              "      <td>0.0175</td>\n",
              "      <td>326</td>\n",
              "      <td>1.400</td>\n",
              "      <td>-0.000001</td>\n",
              "      <td>77.85</td>\n",
              "      <td>4.0125</td>\n",
              "      <td>10.250</td>\n",
              "      <td>77.85</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>30.649500</td>\n",
              "      <td>-10.400</td>\n",
              "      <td>119.0</td>\n",
              "      <td>30.65</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.444</td>\n",
              "      <td>2.760000e-19</td>\n",
              "      <td>327</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.720000e-06</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>Prep</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>176.5</td>\n",
              "      <td>9.9500</td>\n",
              "      <td>-46.000</td>\n",
              "      <td>176.5</td>\n",
              "      <td>10.00</td>\n",
              "      <td>4.00</td>\n",
              "      <td>-0.779</td>\n",
              "      <td>0.0198</td>\n",
              "      <td>328</td>\n",
              "      <td>1.040</td>\n",
              "      <td>2.280000e-06</td>\n",
              "      <td>135.50</td>\n",
              "      <td>4.0125</td>\n",
              "      <td>10.250</td>\n",
              "      <td>135.50</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>326</td>\n",
              "      <td>0.144</td>\n",
              "      <td>-0.000001</td>\n",
              "      <td>77.85</td>\n",
              "      <td>3.9875</td>\n",
              "      <td>0.875</td>\n",
              "      <td>77.85</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>30.650500</td>\n",
              "      <td>8.560</td>\n",
              "      <td>124.0</td>\n",
              "      <td>30.65</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.400</td>\n",
              "      <td>2.760000e-19</td>\n",
              "      <td>328</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.770000e-07</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>Prep</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>177.0</td>\n",
              "      <td>10.1500</td>\n",
              "      <td>54.000</td>\n",
              "      <td>177.0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>4.00</td>\n",
              "      <td>0.979</td>\n",
              "      <td>0.0192</td>\n",
              "      <td>327</td>\n",
              "      <td>2.620</td>\n",
              "      <td>1.150000e-06</td>\n",
              "      <td>135.50</td>\n",
              "      <td>3.9875</td>\n",
              "      <td>0.875</td>\n",
              "      <td>135.50</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.5100</td>\n",
              "      <td>0.0277</td>\n",
              "      <td>326</td>\n",
              "      <td>2.180</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>77.85</td>\n",
              "      <td>3.9875</td>\n",
              "      <td>0.875</td>\n",
              "      <td>77.85</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>30.700000</td>\n",
              "      <td>12.025</td>\n",
              "      <td>124.0</td>\n",
              "      <td>30.65</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.267</td>\n",
              "      <td>2.760000e-19</td>\n",
              "      <td>327</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-3.780000e-07</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>Prep</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>177.5</td>\n",
              "      <td>9.9625</td>\n",
              "      <td>-30.350</td>\n",
              "      <td>177.5</td>\n",
              "      <td>10.00</td>\n",
              "      <td>4.00</td>\n",
              "      <td>-0.779</td>\n",
              "      <td>0.0179</td>\n",
              "      <td>327</td>\n",
              "      <td>1.840</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>135.50</td>\n",
              "      <td>4.0000</td>\n",
              "      <td>4.000</td>\n",
              "      <td>135.50</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0251</td>\n",
              "      <td>0.0250</td>\n",
              "      <td>326</td>\n",
              "      <td>0.642</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>77.85</td>\n",
              "      <td>3.9875</td>\n",
              "      <td>-2.250</td>\n",
              "      <td>77.85</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>30.650500</td>\n",
              "      <td>-23.400</td>\n",
              "      <td>129.0</td>\n",
              "      <td>30.65</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.443</td>\n",
              "      <td>2.760000e-19</td>\n",
              "      <td>327</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>Prep</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>178.5</td>\n",
              "      <td>9.8375</td>\n",
              "      <td>-77.250</td>\n",
              "      <td>178.5</td>\n",
              "      <td>10.00</td>\n",
              "      <td>4.00</td>\n",
              "      <td>-1.260</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>328</td>\n",
              "      <td>1.350</td>\n",
              "      <td>2.560000e-07</td>\n",
              "      <td>135.50</td>\n",
              "      <td>4.0000</td>\n",
              "      <td>7.125</td>\n",
              "      <td>135.50</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.0582</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>327</td>\n",
              "      <td>1.580</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>77.85</td>\n",
              "      <td>3.9875</td>\n",
              "      <td>4.000</td>\n",
              "      <td>77.85</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>30.599125</td>\n",
              "      <td>-16.069</td>\n",
              "      <td>129.0</td>\n",
              "      <td>30.65</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.865</td>\n",
              "      <td>2.760000e-19</td>\n",
              "      <td>327</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-4.070000e-06</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>Prep</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>457</th>\n",
              "      <td>154.0</td>\n",
              "      <td>9.9000</td>\n",
              "      <td>19.650</td>\n",
              "      <td>154.0</td>\n",
              "      <td>9.95</td>\n",
              "      <td>6.92</td>\n",
              "      <td>0.555</td>\n",
              "      <td>0.0226</td>\n",
              "      <td>327</td>\n",
              "      <td>2.220</td>\n",
              "      <td>-1.150000e-06</td>\n",
              "      <td>102.35</td>\n",
              "      <td>2.9500</td>\n",
              "      <td>4.000</td>\n",
              "      <td>102.35</td>\n",
              "      <td>3.07</td>\n",
              "      <td>21.9</td>\n",
              "      <td>-1.0200</td>\n",
              "      <td>0.0255</td>\n",
              "      <td>326</td>\n",
              "      <td>2.350</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>55.20</td>\n",
              "      <td>28.9375</td>\n",
              "      <td>-5.375</td>\n",
              "      <td>55.40</td>\n",
              "      <td>29.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>384.0</td>\n",
              "      <td>30.650875</td>\n",
              "      <td>-0.466</td>\n",
              "      <td>384.0</td>\n",
              "      <td>30.65</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.332</td>\n",
              "      <td>2.770000e-19</td>\n",
              "      <td>326</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>End</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>458</th>\n",
              "      <td>154.5</td>\n",
              "      <td>9.6500</td>\n",
              "      <td>22.750</td>\n",
              "      <td>154.5</td>\n",
              "      <td>10.00</td>\n",
              "      <td>4.00</td>\n",
              "      <td>1.670</td>\n",
              "      <td>0.0268</td>\n",
              "      <td>327</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>102.30</td>\n",
              "      <td>3.9875</td>\n",
              "      <td>-2.250</td>\n",
              "      <td>102.30</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.2540</td>\n",
              "      <td>0.0228</td>\n",
              "      <td>325</td>\n",
              "      <td>3.800</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>57.70</td>\n",
              "      <td>29.0000</td>\n",
              "      <td>16.500</td>\n",
              "      <td>57.90</td>\n",
              "      <td>29.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>389.0</td>\n",
              "      <td>30.750500</td>\n",
              "      <td>32.775</td>\n",
              "      <td>389.0</td>\n",
              "      <td>30.65</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.518</td>\n",
              "      <td>2.770000e-19</td>\n",
              "      <td>326</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>End</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>459</th>\n",
              "      <td>155.0</td>\n",
              "      <td>9.8750</td>\n",
              "      <td>-64.600</td>\n",
              "      <td>155.0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>4.00</td>\n",
              "      <td>0.544</td>\n",
              "      <td>0.0274</td>\n",
              "      <td>327</td>\n",
              "      <td>1.510</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>102.30</td>\n",
              "      <td>4.0500</td>\n",
              "      <td>29.000</td>\n",
              "      <td>102.30</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-1.5300</td>\n",
              "      <td>0.0193</td>\n",
              "      <td>325</td>\n",
              "      <td>1.370</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>60.15</td>\n",
              "      <td>28.9500</td>\n",
              "      <td>13.400</td>\n",
              "      <td>60.40</td>\n",
              "      <td>29.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>389.0</td>\n",
              "      <td>30.700500</td>\n",
              "      <td>17.300</td>\n",
              "      <td>389.0</td>\n",
              "      <td>30.65</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-1.000</td>\n",
              "      <td>2.770000e-19</td>\n",
              "      <td>327</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.050000e-06</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>End</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>460</th>\n",
              "      <td>155.5</td>\n",
              "      <td>9.9500</td>\n",
              "      <td>-52.125</td>\n",
              "      <td>155.5</td>\n",
              "      <td>10.00</td>\n",
              "      <td>4.00</td>\n",
              "      <td>1.100</td>\n",
              "      <td>0.0275</td>\n",
              "      <td>327</td>\n",
              "      <td>1.900</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>102.30</td>\n",
              "      <td>3.9625</td>\n",
              "      <td>-11.650</td>\n",
              "      <td>102.30</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.0891</td>\n",
              "      <td>0.0169</td>\n",
              "      <td>325</td>\n",
              "      <td>3.720</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>62.70</td>\n",
              "      <td>29.0625</td>\n",
              "      <td>25.875</td>\n",
              "      <td>62.90</td>\n",
              "      <td>29.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>394.0</td>\n",
              "      <td>30.649125</td>\n",
              "      <td>2.686</td>\n",
              "      <td>394.0</td>\n",
              "      <td>30.65</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>2.770000e-19</td>\n",
              "      <td>326</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.930000e-06</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>End</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>461</th>\n",
              "      <td>156.0</td>\n",
              "      <td>10.1000</td>\n",
              "      <td>76.125</td>\n",
              "      <td>156.0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>4.00</td>\n",
              "      <td>0.507</td>\n",
              "      <td>0.0192</td>\n",
              "      <td>327</td>\n",
              "      <td>0.363</td>\n",
              "      <td>-1.250000e-06</td>\n",
              "      <td>102.30</td>\n",
              "      <td>3.9500</td>\n",
              "      <td>-17.875</td>\n",
              "      <td>102.30</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.8890</td>\n",
              "      <td>0.0211</td>\n",
              "      <td>325</td>\n",
              "      <td>2.940</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>65.20</td>\n",
              "      <td>29.0250</td>\n",
              "      <td>10.250</td>\n",
              "      <td>65.40</td>\n",
              "      <td>29.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>394.0</td>\n",
              "      <td>30.750500</td>\n",
              "      <td>25.925</td>\n",
              "      <td>394.0</td>\n",
              "      <td>30.65</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.518</td>\n",
              "      <td>2.770000e-19</td>\n",
              "      <td>326</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.410000e-06</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>End</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>462 rows × 48 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     X_ActualPosition  X_ActualVelocity  ...  M_CURRENT_FEEDRATE  Machining_Process\n",
              "0               176.0           10.0500  ...                   6               Prep\n",
              "1               176.5            9.9500  ...                   6               Prep\n",
              "2               177.0           10.1500  ...                   6               Prep\n",
              "3               177.5            9.9625  ...                   6               Prep\n",
              "4               178.5            9.8375  ...                   6               Prep\n",
              "..                ...               ...  ...                 ...                ...\n",
              "457             154.0            9.9000  ...                  50                End\n",
              "458             154.5            9.6500  ...                  50                End\n",
              "459             155.0            9.8750  ...                  50                End\n",
              "460             155.5            9.9500  ...                  50                End\n",
              "461             156.0           10.1000  ...                  50                End\n",
              "\n",
              "[462 rows x 48 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZNxOO6AWnxK",
        "outputId": "d185943c-d72d-4cd8-cbee-7c942699a066"
      },
      "source": [
        "# path에 있는 25개의 개별 생산 단위 데이터 파일의 경로 확인\n",
        "print(all_files)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_10.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_11.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_08.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_02.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_05.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_13.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_17.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_09.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_03.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_04.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_12.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_15.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_14.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_16.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_07.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_01.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_06.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_18.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_20.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_25.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_21.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_24.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_19.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_22.csv', '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_23.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEUP9u_xWnxM"
      },
      "source": [
        "# <2-2 데이터 개수 확인>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aUS_7lmWnxN",
        "outputId": "497816f8-4801-45fb-92c1-dc52b4793533"
      },
      "source": [
        "# count the number of pass/fail items\n",
        "nb_pass = 0\n",
        "nb_pass_half = 0\n",
        "nb_defective = 0\n",
        "for i in range(len(train_sample_np)):\n",
        "    if train_sample_np[i,5] == 'no':\n",
        "        nb_defective += 1\n",
        "    if train_sample_np[i,5] == 'yes' and train_sample_np[i,6] =='yes':\n",
        "        nb_pass += 1\n",
        "    if train_sample_np[i,5] == 'yes' and train_sample_np[i,6] == 'no':\n",
        "        nb_pass_half += 1\n",
        "        \n",
        "print('양품 샘플 개수 : ', nb_pass)\n",
        "print('공정 마쳤으나 육안검사 통과 못한 샘플 개수 : ', nb_pass_half)\n",
        "print('공정 중지된 샘플 개수 : ', nb_defective)\n",
        "print('전체 샘플 개수 : ', nb_pass + nb_pass_half + nb_defective)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "양품 샘플 개수 :  13\n",
            "공정 마쳤으나 육안검사 통과 못한 샘플 개수 :  6\n",
            "공정 중지된 샘플 개수 :  6\n",
            "전체 샘플 개수 :  25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-xCcG2kWnxO"
      },
      "source": [
        "# < 3-1. 사용자 정의 함수 선언 >"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V9Cr71MWnxP"
      },
      "source": [
        "def tool_condition(input):\n",
        "    for i in range(len(input)):\n",
        "        if input[i,4] == 'unworn':\n",
        "            input[i,4] = 0\n",
        "        else:\n",
        "            input[i,4] = 1\n",
        "    return input\n",
        "\n",
        "def item_inspection(input):\n",
        "    for i in range(len(input)):\n",
        "        if input[i,5] == 'no':\n",
        "            input[i,6] = 2\n",
        "        elif input[i,5] == 'yes' and input[i,6] == 'no':\n",
        "            input[i,6] = 1\n",
        "        elif input[i,5] == 'yes' and input[i,6] == 'yes':\n",
        "            input[i,6] = 0\n",
        "    return input\n",
        "\n",
        "\n",
        "def machining_process(input):\n",
        "    for i in range(len(input)):\n",
        "        if input[i,47] == 'Prep':\n",
        "            input[i,47] = 0\n",
        "        elif input[i,47] == 'Layer 1 Up':\n",
        "            input[i,47] = 1\n",
        "        elif input[i,47] == 'Layer 1 Down':\n",
        "            input[i,47] = 2\n",
        "        elif input[i,47] == 'Layer 2 Up':\n",
        "            input[i,47] = 3\n",
        "        elif input[i,47] == 'Layer 2 Down':\n",
        "            input[i,47] = 4\n",
        "        elif input[i,47] == 'Layer 3 Up':\n",
        "            input[i,47] = 5\n",
        "        elif input[i,47] == 'Layer 3 Down':\n",
        "            input[i,47] = 6\n",
        "        elif input[i,47] == 'Repositioning':\n",
        "            input[i,47] = 7\n",
        "        elif input[i,47] == 'End' or 'end':\n",
        "            input[i,47] = 8        \n",
        "        elif input[i,47] == 'Starting':\n",
        "            input[i,47] = 9\n",
        "    return input\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2mhNxCFWnxQ"
      },
      "source": [
        "# <3-2. 사용자 정의 함수의 사용을 통한 데이터 분류>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAG_Qe1JWnxR",
        "outputId": "3f94740a-4b9b-4991-9562-6504cb8dfa72"
      },
      "source": [
        "# Modifying train.csv for training, \n",
        "# - [tool_condition]  : unworn/worn -> 0 / 1 \n",
        "# - [item_inspection] : machining_finalized & passed -> yes & yes / yes & no / no : 0 / 1 / 2\n",
        "# - delete 'material' column and 'No' column\n",
        "\n",
        "train_sample_info = np.array(train_sample_np.copy())\n",
        "train_sample_info = tool_condition(train_sample_info)\n",
        "train_sample_info = item_inspection(train_sample_info)\n",
        "print(train_sample_info)\n",
        "\n",
        "\n",
        "#[feedrate  clamp_pressure  unworn/worn  label]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 'aluminum' 6 4.0 0 'yes' 0]\n",
            " [2 'aluminum' 20 4.0 0 'yes' 0]\n",
            " [3 'aluminum' 6 3.0 0 'yes' 0]\n",
            " [4 'aluminum' 6 2.5 0 'no' 2]\n",
            " [5 'aluminum' 20 3.0 0 'no' 2]\n",
            " [6 'aluminum' 6 4.0 1 'yes' 1]\n",
            " [7 'aluminum' 20 4.0 1 'no' 2]\n",
            " [8 'aluminum' 20 4.0 1 'yes' 1]\n",
            " [9 'aluminum' 15 4.0 1 'yes' 1]\n",
            " [10 'aluminum' 12 4.0 1 'yes' 1]\n",
            " [11 'aluminum' 3 4.0 0 'yes' 0]\n",
            " [12 'aluminum' 3 3.0 0 'yes' 0]\n",
            " [13 'aluminum' 3 4.0 1 'yes' 0]\n",
            " [14 'aluminum' 3 3.0 1 'yes' 0]\n",
            " [15 'aluminum' 6 3.0 1 'yes' 0]\n",
            " [16 'aluminum' 20 3.0 1 'no' 2]\n",
            " [17 'aluminum' 3 2.5 0 'yes' 0]\n",
            " [18 'aluminum' 3 2.5 1 'yes' 0]\n",
            " [19 'aluminum' 15 4.0 1 'yes' 1]\n",
            " [20 'aluminum' 12 4.0 0 'no' 2]\n",
            " [21 'aluminum' 3 4.0 0 'yes' 1]\n",
            " [22 'aluminum' 20 3.0 1 'yes' 0]\n",
            " [23 'aluminum' 3 4.0 1 'no' 2]\n",
            " [24 'aluminum' 3 3.0 0 'yes' 0]\n",
            " [25 'aluminum' 6 2.5 1 'yes' 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f2ErkaNWnxS",
        "outputId": "3d36072a-521e-45a5-8ff0-c27428109b10"
      },
      "source": [
        "train_sample_info = np.delete(train_sample_info,5,1)\n",
        "train_sample_info = np.delete(train_sample_info,0,1)\n",
        "train_sample_info = np.delete(train_sample_info,0,1)\n",
        "print(train_sample_info)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6 4.0 0 0]\n",
            " [20 4.0 0 0]\n",
            " [6 3.0 0 0]\n",
            " [6 2.5 0 2]\n",
            " [20 3.0 0 2]\n",
            " [6 4.0 1 1]\n",
            " [20 4.0 1 2]\n",
            " [20 4.0 1 1]\n",
            " [15 4.0 1 1]\n",
            " [12 4.0 1 1]\n",
            " [3 4.0 0 0]\n",
            " [3 3.0 0 0]\n",
            " [3 4.0 1 0]\n",
            " [3 3.0 1 0]\n",
            " [6 3.0 1 0]\n",
            " [20 3.0 1 2]\n",
            " [3 2.5 0 0]\n",
            " [3 2.5 1 0]\n",
            " [15 4.0 1 1]\n",
            " [12 4.0 0 2]\n",
            " [3 4.0 0 1]\n",
            " [20 3.0 1 0]\n",
            " [3 4.0 1 2]\n",
            " [3 3.0 0 0]\n",
            " [6 2.5 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krQtZVi1fLJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2420c8a5-cd65-45ab-bdc4-63a30d9609be"
      },
      "source": [
        "all_files.sort()\n",
        "all_files"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_01.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_02.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_03.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_04.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_05.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_06.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_07.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_08.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_09.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_10.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_11.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_12.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_13.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_14.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_15.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_16.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_17.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_18.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_19.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_20.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_21.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_22.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_23.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_24.csv',\n",
              " '/content/drive/MyDrive/Colab Notebooks/제조프로젝트/dataset/CNC 비식별화 원본데이터_1209/CNC Virtual Data set _v2/experiment_25.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpD2UUg9WnxS",
        "outputId": "2b504ab0-e4a4-492c-c375-5a5fac8f1ef7"
      },
      "source": [
        "k  = 0\n",
        "li_pass = []\n",
        "li_pass_half = []\n",
        "li_fail = []\n",
        "\n",
        "for filename in all_files:\n",
        "    df = pd.read_csv(filename, index_col=None, header=0)      \n",
        "    \n",
        "    if train_sample_info[k,3] == 0:\n",
        "        li_pass.append(df)        \n",
        "    elif train_sample_info[k,3] == 1:\n",
        "        li_pass_half.append(df)        \n",
        "    else :\n",
        "        li_fail.append(df)\n",
        "        \n",
        "    k += 1\n",
        "    \n",
        "frame01 = pd.concat(li_pass, axis=0, ignore_index=True)\n",
        "frame02 = pd.concat(li_pass_half, axis=0, ignore_index=True)\n",
        "frame03 = pd.concat(li_fail, axis=0, ignore_index=True)\n",
        "\n",
        "data_pass = np.array(frame01.copy())\n",
        "data_pass_half = np.array(frame02.copy())\n",
        "data_fail = np.array(frame03.copy())\n",
        "\n",
        "\n",
        "print('공정완료 및 육안검사 합격한 전체 데이터 수 : ',len(data_pass))\n",
        "print('공정완료 및 육안검사 불합격한 전체 데이터 수 : ',len(data_pass_half))\n",
        "print('공정 미완료한 전체 데이터 수 : ',len(data_fail))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "공정완료 및 육안검사 합격한 전체 데이터 수 :  22645\n",
            "공정완료 및 육안검사 불합격한 전체 데이터 수 :  6175\n",
            "공정 미완료한 전체 데이터 수 :  3228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvXH2-uGWnxU",
        "outputId": "b3aeb96e-36bb-44e5-ed23-38f0f2343828"
      },
      "source": [
        "print(data_pass.shape)\n",
        "print(data_pass_half.shape)\n",
        "print(data_fail.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(22645, 48)\n",
            "(6175, 48)\n",
            "(3228, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfdpMIn5WnxV"
      },
      "source": [
        "# Modifying experiment data \n",
        "#  - machining_process : From \"Prep\" to \"End\" -> 0~9\n",
        "data_pass = machining_process(data_pass)\n",
        "data_pass_half = machining_process(data_pass_half)\n",
        "data_fail = machining_process(data_fail)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ9z4RP5mC3E",
        "outputId": "c6ecec7b-32bf-443f-caea-a73bb4d68760"
      },
      "source": [
        "data_pass"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[202.0, 4.0, 4.0, ..., 0, 50, 8],\n",
              "       [202.0, -6.8, -346.0, ..., 4, 50, 0],\n",
              "       [200.0, -13.8, -2.25, ..., 7, 50, 0],\n",
              "       ...,\n",
              "       [178.5, 3.5475, 17.5, ..., 0, 20, 8],\n",
              "       [178.5, 3.4475, 11.25, ..., 0, 20, 8],\n",
              "       [178.0, 3.5625, 1.85, ..., 0, 20, 8]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HEL-nryWnxV"
      },
      "source": [
        "# <3-3. 데이터셋 구성>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK8RKX6VWnxW"
      },
      "source": [
        "# label 0/1 --> data01 / data02+data03\n",
        "\n",
        "data01 = data_pass[0:3228+6175,:]\n",
        "data02 = data_pass_half[0:6175,:]\n",
        "data03 = data_fail[0:3228,:]\n",
        "\n",
        "data = np.concatenate((data01,data02),axis=0);\n",
        "data = np.concatenate((data,data03),axis=0);\n",
        "\n",
        "data_all= data_pass[3228+6175:22645,:]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGDgoSyTWnxW",
        "outputId": "14f62ca9-53a4-446a-b3bf-81b4899cea6e"
      },
      "source": [
        "print((data))\n",
        "print(data.shape)\n",
        "print(data_all.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[202.0 4.0 4.0 ... 0 50 8]\n",
            " [202.0 -6.8 -346.0 ... 4 50 0]\n",
            " [200.0 -13.8 -2.25 ... 7 50 0]\n",
            " ...\n",
            " [155.0 9.875 -64.6 ... 0 50 8]\n",
            " [155.5 9.95 -52.125 ... 0 50 8]\n",
            " [156.0 10.1 76.125 ... 0 50 8]]\n",
            "(18806, 48)\n",
            "(13242, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NCvNm1nWnxX"
      },
      "source": [
        "# <3-4. 데이터 정제 (2차 전처리)>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ollC_m1-WnxX"
      },
      "source": [
        "# 2차 전처리 진행, MinMaxScaler 사용\n",
        "sc = MinMaxScaler()\n",
        "X_train = sc.fit_transform(data) \n",
        "X_train = np.array(X_train)\n",
        "X_test = sc.fit_transform(data_all)\n",
        "X_test = np.array(X_test)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtAzeyOJWnxY"
      },
      "source": [
        "# <4-1. 라벨 데이터 제작>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNK6YfJ-WnxY",
        "outputId": "7ddea90f-2469-43f1-fd44-8e5f69d30620"
      },
      "source": [
        "# 라벨 데이터 생성\n",
        "\n",
        "Y_train = np.zeros((len(X_train),1),dtype='int')\n",
        "Y_test = np.zeros((len(X_test),1),dtype='int')\n",
        "l = int(Y_train.shape[0]/2)\n",
        "\n",
        "Y_train[0:l,:] = 0\n",
        "Y_train[l:l*2,:] = 1\n",
        "\n",
        "print(Y_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EBNeEcmWnxZ"
      },
      "source": [
        "# <5-1. 학습/검증/평가 데이터 구성>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6aG8g6TWnxZ"
      },
      "source": [
        "# 뒤의 <7-1. AI 모델 훈련> 항목에 포함되어 있음"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gShEQWbgWnxZ"
      },
      "source": [
        "# <6-1. AI 모델 파라미터 설정>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JsIxIHxWnxZ"
      },
      "source": [
        "nb_classes = 2\n",
        "batch_size = 1024\n",
        "epochs = 300\n",
        "lr = 1e-4"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NogwSqnaWnxa"
      },
      "source": [
        "# <6-2. 데이터셋 준비>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB6G9yFnWnxa",
        "outputId": "53804dab-8d41-4710-e564-3ee0e4d2b0c2"
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(Y_test, nb_classes)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18806, 48)\n",
            "(13242, 48)\n",
            "(18806, 2)\n",
            "(13242, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHgRfq_6Wnxa"
      },
      "source": [
        "# <6-3. AI 모델 디자인>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS6rdyv8Wnxb",
        "outputId": "ddb2b279-8f5e-4441-eca0-ffbca835a039"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_dim=48))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(256, activation='relu')) # 깊이가 깊어서 sigmoid를 쓰면 0에 수렴할 확률이 있기 때문에 relu 활성함수를 사용하는 것\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(nb_classes, activation='sigmoid')) # 양품 확률 출력하기 위해 확률로 노드1개 -> dense =1 \n",
        "\n",
        "model_checkpoint = ModelCheckpoint('weight_CNC_binary.mat', monitor='val_acc',save_best_only=True)\n",
        "opt=Adam(lr)\n",
        "model.summary()\n",
        "model.compile(optimizer=opt,loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = History()\n",
        "print('.............................model is defined.............................')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:127: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3144: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 128)               6272      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 598,018\n",
            "Trainable params: 598,018\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3075: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            ".............................model is defined.............................\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVUssmV5Wnxb"
      },
      "source": [
        "# <7-1. AI 모델 훈련>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wRe1oDAWnxc",
        "outputId": "ae63ab8f-6762-4a29-c614-38c4cff24d31"
      },
      "source": [
        "model.fit(X_train, Y_train, verbose=2, batch_size=batch_size, epochs=epochs, validation_split=0.1, shuffle=True, callbacks=[history])\n",
        "model.save_weights('weight_CNC_binary.mat')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:977: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:964: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2503: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 16925 samples, validate on 1881 samples\n",
            "Epoch 1/300\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:184: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:193: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:200: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " - 3s - loss: 0.6896 - acc: 0.5311 - val_loss: 0.7622 - val_acc: 0.0000e+00\n",
            "Epoch 2/300\n",
            " - 2s - loss: 0.6735 - acc: 0.5659 - val_loss: 0.7578 - val_acc: 0.4094\n",
            "Epoch 3/300\n",
            " - 2s - loss: 0.6531 - acc: 0.6061 - val_loss: 0.6837 - val_acc: 0.6701\n",
            "Epoch 4/300\n",
            " - 2s - loss: 0.6300 - acc: 0.6604 - val_loss: 0.6312 - val_acc: 0.7350\n",
            "Epoch 5/300\n",
            " - 2s - loss: 0.6092 - acc: 0.6745 - val_loss: 0.5906 - val_acc: 0.8432\n",
            "Epoch 6/300\n",
            " - 2s - loss: 0.5765 - acc: 0.6941 - val_loss: 0.5188 - val_acc: 0.9009\n",
            "Epoch 7/300\n",
            " - 2s - loss: 0.5461 - acc: 0.7178 - val_loss: 0.4490 - val_acc: 0.9585\n",
            "Epoch 8/300\n",
            " - 2s - loss: 0.5181 - acc: 0.7372 - val_loss: 0.4185 - val_acc: 0.9779\n",
            "Epoch 9/300\n",
            " - 2s - loss: 0.4956 - acc: 0.7536 - val_loss: 0.4276 - val_acc: 0.9702\n",
            "Epoch 10/300\n",
            " - 2s - loss: 0.4743 - acc: 0.7694 - val_loss: 0.3882 - val_acc: 0.9838\n",
            "Epoch 11/300\n",
            " - 2s - loss: 0.4543 - acc: 0.7840 - val_loss: 0.4009 - val_acc: 0.8514\n",
            "Epoch 12/300\n",
            " - 2s - loss: 0.4339 - acc: 0.7980 - val_loss: 0.3999 - val_acc: 0.7650\n",
            "Epoch 13/300\n",
            " - 2s - loss: 0.4154 - acc: 0.8112 - val_loss: 0.3708 - val_acc: 0.7812\n",
            "Epoch 14/300\n",
            " - 2s - loss: 0.4089 - acc: 0.8153 - val_loss: 0.3327 - val_acc: 0.8456\n",
            "Epoch 15/300\n",
            " - 2s - loss: 0.3916 - acc: 0.8268 - val_loss: 0.3779 - val_acc: 0.7690\n",
            "Epoch 16/300\n",
            " - 2s - loss: 0.3748 - acc: 0.8352 - val_loss: 0.3087 - val_acc: 0.8437\n",
            "Epoch 17/300\n",
            " - 2s - loss: 0.3601 - acc: 0.8456 - val_loss: 0.3217 - val_acc: 0.7937\n",
            "Epoch 18/300\n",
            " - 2s - loss: 0.3585 - acc: 0.8447 - val_loss: 0.3512 - val_acc: 0.7751\n",
            "Epoch 19/300\n",
            " - 2s - loss: 0.3443 - acc: 0.8521 - val_loss: 0.3178 - val_acc: 0.8060\n",
            "Epoch 20/300\n",
            " - 2s - loss: 0.3338 - acc: 0.8557 - val_loss: 0.3670 - val_acc: 0.7709\n",
            "Epoch 21/300\n",
            " - 2s - loss: 0.3225 - acc: 0.8638 - val_loss: 0.3600 - val_acc: 0.7714\n",
            "Epoch 22/300\n",
            " - 2s - loss: 0.3129 - acc: 0.8666 - val_loss: 0.3281 - val_acc: 0.7799\n",
            "Epoch 23/300\n",
            " - 2s - loss: 0.2970 - acc: 0.8750 - val_loss: 0.3169 - val_acc: 0.7828\n",
            "Epoch 24/300\n",
            " - 2s - loss: 0.2886 - acc: 0.8771 - val_loss: 0.3591 - val_acc: 0.7709\n",
            "Epoch 25/300\n",
            " - 2s - loss: 0.2815 - acc: 0.8816 - val_loss: 0.3434 - val_acc: 0.7775\n",
            "Epoch 26/300\n",
            " - 2s - loss: 0.2786 - acc: 0.8814 - val_loss: 0.3621 - val_acc: 0.7719\n",
            "Epoch 27/300\n",
            " - 2s - loss: 0.2658 - acc: 0.8883 - val_loss: 0.3872 - val_acc: 0.7725\n",
            "Epoch 28/300\n",
            " - 2s - loss: 0.2539 - acc: 0.8913 - val_loss: 0.3915 - val_acc: 0.7754\n",
            "Epoch 29/300\n",
            " - 2s - loss: 0.2486 - acc: 0.8960 - val_loss: 0.3557 - val_acc: 0.7786\n",
            "Epoch 30/300\n",
            " - 2s - loss: 0.2439 - acc: 0.8982 - val_loss: 0.3516 - val_acc: 0.7879\n",
            "Epoch 31/300\n",
            " - 2s - loss: 0.2326 - acc: 0.9013 - val_loss: 0.3766 - val_acc: 0.7770\n",
            "Epoch 32/300\n",
            " - 2s - loss: 0.2312 - acc: 0.9010 - val_loss: 0.3589 - val_acc: 0.7982\n",
            "Epoch 33/300\n",
            " - 2s - loss: 0.2280 - acc: 0.9059 - val_loss: 0.3909 - val_acc: 0.7850\n",
            "Epoch 34/300\n",
            " - 2s - loss: 0.2207 - acc: 0.9072 - val_loss: 0.3706 - val_acc: 0.8219\n",
            "Epoch 35/300\n",
            " - 2s - loss: 0.2169 - acc: 0.9108 - val_loss: 0.3695 - val_acc: 0.8256\n",
            "Epoch 36/300\n",
            " - 2s - loss: 0.2114 - acc: 0.9128 - val_loss: 0.3893 - val_acc: 0.8235\n",
            "Epoch 37/300\n",
            " - 2s - loss: 0.2096 - acc: 0.9124 - val_loss: 0.3830 - val_acc: 0.8246\n",
            "Epoch 38/300\n",
            " - 2s - loss: 0.2012 - acc: 0.9173 - val_loss: 0.3729 - val_acc: 0.8278\n",
            "Epoch 39/300\n",
            " - 2s - loss: 0.1987 - acc: 0.9193 - val_loss: 0.3800 - val_acc: 0.8315\n",
            "Epoch 40/300\n",
            " - 2s - loss: 0.1966 - acc: 0.9184 - val_loss: 0.4405 - val_acc: 0.8216\n",
            "Epoch 41/300\n",
            " - 2s - loss: 0.1899 - acc: 0.9224 - val_loss: 0.3499 - val_acc: 0.8432\n",
            "Epoch 42/300\n",
            " - 2s - loss: 0.1900 - acc: 0.9212 - val_loss: 0.4198 - val_acc: 0.8246\n",
            "Epoch 43/300\n",
            " - 2s - loss: 0.1819 - acc: 0.9267 - val_loss: 0.4004 - val_acc: 0.8315\n",
            "Epoch 44/300\n",
            " - 2s - loss: 0.1791 - acc: 0.9277 - val_loss: 0.4558 - val_acc: 0.8203\n",
            "Epoch 45/300\n",
            " - 2s - loss: 0.1772 - acc: 0.9280 - val_loss: 0.4202 - val_acc: 0.8283\n",
            "Epoch 46/300\n",
            " - 2s - loss: 0.1802 - acc: 0.9268 - val_loss: 0.4480 - val_acc: 0.8230\n",
            "Epoch 47/300\n",
            " - 2s - loss: 0.1724 - acc: 0.9294 - val_loss: 0.4824 - val_acc: 0.8203\n",
            "Epoch 48/300\n",
            " - 2s - loss: 0.1694 - acc: 0.9317 - val_loss: 0.5081 - val_acc: 0.8073\n",
            "Epoch 49/300\n",
            " - 2s - loss: 0.1684 - acc: 0.9303 - val_loss: 0.5061 - val_acc: 0.8004\n",
            "Epoch 50/300\n",
            " - 2s - loss: 0.1634 - acc: 0.9359 - val_loss: 0.5668 - val_acc: 0.7709\n",
            "Epoch 51/300\n",
            " - 2s - loss: 0.1622 - acc: 0.9346 - val_loss: 0.5546 - val_acc: 0.7709\n",
            "Epoch 52/300\n",
            " - 2s - loss: 0.1612 - acc: 0.9334 - val_loss: 0.5480 - val_acc: 0.7916\n",
            "Epoch 53/300\n",
            " - 2s - loss: 0.1578 - acc: 0.9339 - val_loss: 0.5550 - val_acc: 0.7733\n",
            "Epoch 54/300\n",
            " - 2s - loss: 0.1536 - acc: 0.9373 - val_loss: 0.5959 - val_acc: 0.7711\n",
            "Epoch 55/300\n",
            " - 2s - loss: 0.1538 - acc: 0.9362 - val_loss: 0.5627 - val_acc: 0.7730\n",
            "Epoch 56/300\n",
            " - 2s - loss: 0.1489 - acc: 0.9410 - val_loss: 0.5998 - val_acc: 0.7741\n",
            "Epoch 57/300\n",
            " - 2s - loss: 0.1482 - acc: 0.9396 - val_loss: 0.5978 - val_acc: 0.7751\n",
            "Epoch 58/300\n",
            " - 2s - loss: 0.1478 - acc: 0.9399 - val_loss: 0.6325 - val_acc: 0.7709\n",
            "Epoch 59/300\n",
            " - 2s - loss: 0.1407 - acc: 0.9426 - val_loss: 0.6406 - val_acc: 0.7709\n",
            "Epoch 60/300\n",
            " - 2s - loss: 0.1452 - acc: 0.9406 - val_loss: 0.6518 - val_acc: 0.7650\n",
            "Epoch 61/300\n",
            " - 2s - loss: 0.1383 - acc: 0.9433 - val_loss: 0.7132 - val_acc: 0.7634\n",
            "Epoch 62/300\n",
            " - 2s - loss: 0.1349 - acc: 0.9440 - val_loss: 0.6978 - val_acc: 0.7610\n",
            "Epoch 63/300\n",
            " - 2s - loss: 0.1373 - acc: 0.9432 - val_loss: 0.6835 - val_acc: 0.7624\n",
            "Epoch 64/300\n",
            " - 2s - loss: 0.1318 - acc: 0.9438 - val_loss: 0.7274 - val_acc: 0.7608\n",
            "Epoch 65/300\n",
            " - 2s - loss: 0.1352 - acc: 0.9440 - val_loss: 0.7198 - val_acc: 0.7602\n",
            "Epoch 66/300\n",
            " - 2s - loss: 0.1351 - acc: 0.9449 - val_loss: 0.7583 - val_acc: 0.7578\n",
            "Epoch 67/300\n",
            " - 2s - loss: 0.1324 - acc: 0.9449 - val_loss: 0.7297 - val_acc: 0.7608\n",
            "Epoch 68/300\n",
            " - 2s - loss: 0.1258 - acc: 0.9465 - val_loss: 0.7375 - val_acc: 0.7608\n",
            "Epoch 69/300\n",
            " - 2s - loss: 0.1289 - acc: 0.9468 - val_loss: 0.7667 - val_acc: 0.7616\n",
            "Epoch 70/300\n",
            " - 2s - loss: 0.1329 - acc: 0.9459 - val_loss: 0.7583 - val_acc: 0.7618\n",
            "Epoch 71/300\n",
            " - 2s - loss: 0.1283 - acc: 0.9473 - val_loss: 0.7621 - val_acc: 0.7613\n",
            "Epoch 72/300\n",
            " - 2s - loss: 0.1250 - acc: 0.9472 - val_loss: 0.7618 - val_acc: 0.7624\n",
            "Epoch 73/300\n",
            " - 2s - loss: 0.1219 - acc: 0.9473 - val_loss: 0.8174 - val_acc: 0.7600\n",
            "Epoch 74/300\n",
            " - 2s - loss: 0.1205 - acc: 0.9479 - val_loss: 0.8410 - val_acc: 0.7597\n",
            "Epoch 75/300\n",
            " - 2s - loss: 0.1211 - acc: 0.9495 - val_loss: 0.8068 - val_acc: 0.7645\n",
            "Epoch 76/300\n",
            " - 2s - loss: 0.1175 - acc: 0.9518 - val_loss: 0.9122 - val_acc: 0.7565\n",
            "Epoch 77/300\n",
            " - 2s - loss: 0.1171 - acc: 0.9519 - val_loss: 0.8837 - val_acc: 0.7624\n",
            "Epoch 78/300\n",
            " - 2s - loss: 0.1154 - acc: 0.9528 - val_loss: 0.8644 - val_acc: 0.7621\n",
            "Epoch 79/300\n",
            " - 2s - loss: 0.1145 - acc: 0.9537 - val_loss: 0.8656 - val_acc: 0.7618\n",
            "Epoch 80/300\n",
            " - 2s - loss: 0.1118 - acc: 0.9514 - val_loss: 0.8889 - val_acc: 0.7600\n",
            "Epoch 81/300\n",
            " - 2s - loss: 0.1115 - acc: 0.9529 - val_loss: 0.8972 - val_acc: 0.7565\n",
            "Epoch 82/300\n",
            " - 2s - loss: 0.1104 - acc: 0.9529 - val_loss: 0.9273 - val_acc: 0.7592\n",
            "Epoch 83/300\n",
            " - 2s - loss: 0.1120 - acc: 0.9540 - val_loss: 0.9503 - val_acc: 0.7576\n",
            "Epoch 84/300\n",
            " - 2s - loss: 0.1084 - acc: 0.9537 - val_loss: 0.9356 - val_acc: 0.7608\n",
            "Epoch 85/300\n",
            " - 2s - loss: 0.1076 - acc: 0.9542 - val_loss: 0.9454 - val_acc: 0.7610\n",
            "Epoch 86/300\n",
            " - 2s - loss: 0.1092 - acc: 0.9532 - val_loss: 0.9250 - val_acc: 0.7661\n",
            "Epoch 87/300\n",
            " - 2s - loss: 0.1116 - acc: 0.9524 - val_loss: 0.9943 - val_acc: 0.7608\n",
            "Epoch 88/300\n",
            " - 2s - loss: 0.1053 - acc: 0.9565 - val_loss: 1.0264 - val_acc: 0.7570\n",
            "Epoch 89/300\n",
            " - 2s - loss: 0.1049 - acc: 0.9556 - val_loss: 0.9826 - val_acc: 0.7581\n",
            "Epoch 90/300\n",
            " - 2s - loss: 0.1039 - acc: 0.9565 - val_loss: 1.0069 - val_acc: 0.7610\n",
            "Epoch 91/300\n",
            " - 2s - loss: 0.1033 - acc: 0.9568 - val_loss: 1.0862 - val_acc: 0.7544\n",
            "Epoch 92/300\n",
            " - 2s - loss: 0.1012 - acc: 0.9572 - val_loss: 1.1143 - val_acc: 0.7562\n",
            "Epoch 93/300\n",
            " - 2s - loss: 0.0992 - acc: 0.9580 - val_loss: 1.0673 - val_acc: 0.7661\n",
            "Epoch 94/300\n",
            " - 2s - loss: 0.1054 - acc: 0.9552 - val_loss: 1.1258 - val_acc: 0.7552\n",
            "Epoch 95/300\n",
            " - 2s - loss: 0.0985 - acc: 0.9607 - val_loss: 1.0815 - val_acc: 0.7597\n",
            "Epoch 96/300\n",
            " - 2s - loss: 0.1017 - acc: 0.9577 - val_loss: 1.0683 - val_acc: 0.7613\n",
            "Epoch 97/300\n",
            " - 2s - loss: 0.0964 - acc: 0.9600 - val_loss: 1.1710 - val_acc: 0.7600\n",
            "Epoch 98/300\n",
            " - 2s - loss: 0.0944 - acc: 0.9596 - val_loss: 1.1308 - val_acc: 0.7621\n",
            "Epoch 99/300\n",
            " - 2s - loss: 0.0955 - acc: 0.9596 - val_loss: 1.1583 - val_acc: 0.7610\n",
            "Epoch 100/300\n",
            " - 2s - loss: 0.0963 - acc: 0.9586 - val_loss: 1.1476 - val_acc: 0.7576\n",
            "Epoch 101/300\n",
            " - 2s - loss: 0.0906 - acc: 0.9621 - val_loss: 1.1783 - val_acc: 0.7565\n",
            "Epoch 102/300\n",
            " - 2s - loss: 0.0954 - acc: 0.9610 - val_loss: 1.2489 - val_acc: 0.7549\n",
            "Epoch 103/300\n",
            " - 2s - loss: 0.0975 - acc: 0.9581 - val_loss: 1.2969 - val_acc: 0.7544\n",
            "Epoch 104/300\n",
            " - 2s - loss: 0.0927 - acc: 0.9610 - val_loss: 1.1865 - val_acc: 0.7570\n",
            "Epoch 105/300\n",
            " - 2s - loss: 0.0916 - acc: 0.9627 - val_loss: 1.2614 - val_acc: 0.7618\n",
            "Epoch 106/300\n",
            " - 2s - loss: 0.0929 - acc: 0.9618 - val_loss: 1.2738 - val_acc: 0.7613\n",
            "Epoch 107/300\n",
            " - 2s - loss: 0.0889 - acc: 0.9635 - val_loss: 1.3015 - val_acc: 0.7584\n",
            "Epoch 108/300\n",
            " - 2s - loss: 0.0868 - acc: 0.9646 - val_loss: 1.2783 - val_acc: 0.7618\n",
            "Epoch 109/300\n",
            " - 2s - loss: 0.0863 - acc: 0.9640 - val_loss: 1.3864 - val_acc: 0.7613\n",
            "Epoch 110/300\n",
            " - 2s - loss: 0.0924 - acc: 0.9627 - val_loss: 1.3308 - val_acc: 0.7608\n",
            "Epoch 111/300\n",
            " - 2s - loss: 0.0866 - acc: 0.9630 - val_loss: 1.4193 - val_acc: 0.7608\n",
            "Epoch 112/300\n",
            " - 2s - loss: 0.0818 - acc: 0.9653 - val_loss: 1.4269 - val_acc: 0.7581\n",
            "Epoch 113/300\n",
            " - 2s - loss: 0.0837 - acc: 0.9655 - val_loss: 1.4281 - val_acc: 0.7602\n",
            "Epoch 114/300\n",
            " - 2s - loss: 0.0853 - acc: 0.9643 - val_loss: 1.4380 - val_acc: 0.7600\n",
            "Epoch 115/300\n",
            " - 2s - loss: 0.0852 - acc: 0.9635 - val_loss: 1.4786 - val_acc: 0.7549\n",
            "Epoch 116/300\n",
            " - 2s - loss: 0.0860 - acc: 0.9635 - val_loss: 1.4333 - val_acc: 0.7597\n",
            "Epoch 117/300\n",
            " - 2s - loss: 0.0837 - acc: 0.9651 - val_loss: 1.3800 - val_acc: 0.7597\n",
            "Epoch 118/300\n",
            " - 2s - loss: 0.0803 - acc: 0.9674 - val_loss: 1.4455 - val_acc: 0.7589\n",
            "Epoch 119/300\n",
            " - 2s - loss: 0.0832 - acc: 0.9659 - val_loss: 1.5587 - val_acc: 0.7629\n",
            "Epoch 120/300\n",
            " - 2s - loss: 0.0811 - acc: 0.9663 - val_loss: 1.4616 - val_acc: 0.7629\n",
            "Epoch 121/300\n",
            " - 2s - loss: 0.0785 - acc: 0.9683 - val_loss: 1.5850 - val_acc: 0.7610\n",
            "Epoch 122/300\n",
            " - 2s - loss: 0.0794 - acc: 0.9685 - val_loss: 1.4496 - val_acc: 0.7634\n",
            "Epoch 123/300\n",
            " - 2s - loss: 0.0782 - acc: 0.9674 - val_loss: 1.5077 - val_acc: 0.7613\n",
            "Epoch 124/300\n",
            " - 2s - loss: 0.0736 - acc: 0.9712 - val_loss: 1.5472 - val_acc: 0.7610\n",
            "Epoch 125/300\n",
            " - 2s - loss: 0.0756 - acc: 0.9687 - val_loss: 1.6029 - val_acc: 0.7581\n",
            "Epoch 126/300\n",
            " - 2s - loss: 0.0749 - acc: 0.9678 - val_loss: 1.5454 - val_acc: 0.7586\n",
            "Epoch 127/300\n",
            " - 2s - loss: 0.0734 - acc: 0.9695 - val_loss: 1.5880 - val_acc: 0.7608\n",
            "Epoch 128/300\n",
            " - 2s - loss: 0.0738 - acc: 0.9707 - val_loss: 1.5460 - val_acc: 0.7618\n",
            "Epoch 129/300\n",
            " - 2s - loss: 0.0745 - acc: 0.9688 - val_loss: 1.5811 - val_acc: 0.7602\n",
            "Epoch 130/300\n",
            " - 2s - loss: 0.0737 - acc: 0.9699 - val_loss: 1.5884 - val_acc: 0.7613\n",
            "Epoch 131/300\n",
            " - 2s - loss: 0.0683 - acc: 0.9710 - val_loss: 1.5900 - val_acc: 0.7613\n",
            "Epoch 132/300\n",
            " - 2s - loss: 0.0750 - acc: 0.9687 - val_loss: 1.6205 - val_acc: 0.7597\n",
            "Epoch 133/300\n",
            " - 2s - loss: 0.0703 - acc: 0.9711 - val_loss: 1.5987 - val_acc: 0.7618\n",
            "Epoch 134/300\n",
            " - 2s - loss: 0.0681 - acc: 0.9715 - val_loss: 1.7054 - val_acc: 0.7608\n",
            "Epoch 135/300\n",
            " - 2s - loss: 0.0677 - acc: 0.9721 - val_loss: 1.7502 - val_acc: 0.7533\n",
            "Epoch 136/300\n",
            " - 2s - loss: 0.0719 - acc: 0.9705 - val_loss: 1.6023 - val_acc: 0.7613\n",
            "Epoch 137/300\n",
            " - 2s - loss: 0.0703 - acc: 0.9710 - val_loss: 1.6404 - val_acc: 0.7629\n",
            "Epoch 138/300\n",
            " - 2s - loss: 0.0660 - acc: 0.9734 - val_loss: 1.7229 - val_acc: 0.7624\n",
            "Epoch 139/300\n",
            " - 2s - loss: 0.0653 - acc: 0.9733 - val_loss: 1.7280 - val_acc: 0.7624\n",
            "Epoch 140/300\n",
            " - 2s - loss: 0.0669 - acc: 0.9720 - val_loss: 1.7005 - val_acc: 0.7608\n",
            "Epoch 141/300\n",
            " - 2s - loss: 0.0684 - acc: 0.9722 - val_loss: 1.7147 - val_acc: 0.7618\n",
            "Epoch 142/300\n",
            " - 2s - loss: 0.0660 - acc: 0.9721 - val_loss: 1.6932 - val_acc: 0.7613\n",
            "Epoch 143/300\n",
            " - 2s - loss: 0.0647 - acc: 0.9716 - val_loss: 1.7463 - val_acc: 0.7602\n",
            "Epoch 144/300\n",
            " - 2s - loss: 0.0670 - acc: 0.9707 - val_loss: 1.8157 - val_acc: 0.7602\n",
            "Epoch 145/300\n",
            " - 2s - loss: 0.0641 - acc: 0.9727 - val_loss: 1.7976 - val_acc: 0.7586\n",
            "Epoch 146/300\n",
            " - 2s - loss: 0.0653 - acc: 0.9737 - val_loss: 1.8901 - val_acc: 0.7531\n",
            "Epoch 147/300\n",
            " - 2s - loss: 0.0624 - acc: 0.9739 - val_loss: 1.8598 - val_acc: 0.7581\n",
            "Epoch 148/300\n",
            " - 2s - loss: 0.0606 - acc: 0.9750 - val_loss: 1.7396 - val_acc: 0.7581\n",
            "Epoch 149/300\n",
            " - 2s - loss: 0.0676 - acc: 0.9729 - val_loss: 1.7788 - val_acc: 0.7602\n",
            "Epoch 150/300\n",
            " - 2s - loss: 0.0590 - acc: 0.9753 - val_loss: 1.9004 - val_acc: 0.7597\n",
            "Epoch 151/300\n",
            " - 2s - loss: 0.0617 - acc: 0.9748 - val_loss: 1.9347 - val_acc: 0.7570\n",
            "Epoch 152/300\n",
            " - 2s - loss: 0.0585 - acc: 0.9760 - val_loss: 1.8781 - val_acc: 0.7597\n",
            "Epoch 153/300\n",
            " - 2s - loss: 0.0562 - acc: 0.9774 - val_loss: 1.9611 - val_acc: 0.7602\n",
            "Epoch 154/300\n",
            " - 2s - loss: 0.0604 - acc: 0.9755 - val_loss: 2.0033 - val_acc: 0.7592\n",
            "Epoch 155/300\n",
            " - 2s - loss: 0.0583 - acc: 0.9753 - val_loss: 1.9179 - val_acc: 0.7608\n",
            "Epoch 156/300\n",
            " - 2s - loss: 0.0610 - acc: 0.9742 - val_loss: 1.9517 - val_acc: 0.7594\n",
            "Epoch 157/300\n",
            " - 2s - loss: 0.0560 - acc: 0.9775 - val_loss: 2.0286 - val_acc: 0.7560\n",
            "Epoch 158/300\n",
            " - 2s - loss: 0.0582 - acc: 0.9768 - val_loss: 1.9850 - val_acc: 0.7533\n",
            "Epoch 159/300\n",
            " - 2s - loss: 0.0584 - acc: 0.9756 - val_loss: 1.9546 - val_acc: 0.7560\n",
            "Epoch 160/300\n",
            " - 2s - loss: 0.0580 - acc: 0.9764 - val_loss: 1.9170 - val_acc: 0.7584\n",
            "Epoch 161/300\n",
            " - 2s - loss: 0.0574 - acc: 0.9758 - val_loss: 1.9917 - val_acc: 0.7592\n",
            "Epoch 162/300\n",
            " - 2s - loss: 0.0579 - acc: 0.9760 - val_loss: 1.8639 - val_acc: 0.7610\n",
            "Epoch 163/300\n",
            " - 2s - loss: 0.0585 - acc: 0.9777 - val_loss: 1.9304 - val_acc: 0.7602\n",
            "Epoch 164/300\n",
            " - 2s - loss: 0.0532 - acc: 0.9791 - val_loss: 1.9729 - val_acc: 0.7624\n",
            "Epoch 165/300\n",
            " - 2s - loss: 0.0599 - acc: 0.9766 - val_loss: 1.9289 - val_acc: 0.7586\n",
            "Epoch 166/300\n",
            " - 2s - loss: 0.0575 - acc: 0.9760 - val_loss: 1.8571 - val_acc: 0.7592\n",
            "Epoch 167/300\n",
            " - 2s - loss: 0.0561 - acc: 0.9769 - val_loss: 1.9540 - val_acc: 0.7608\n",
            "Epoch 168/300\n",
            " - 2s - loss: 0.0584 - acc: 0.9777 - val_loss: 2.0693 - val_acc: 0.7576\n",
            "Epoch 169/300\n",
            " - 2s - loss: 0.0551 - acc: 0.9768 - val_loss: 1.9632 - val_acc: 0.7592\n",
            "Epoch 170/300\n",
            " - 2s - loss: 0.0535 - acc: 0.9780 - val_loss: 1.9719 - val_acc: 0.7592\n",
            "Epoch 171/300\n",
            " - 2s - loss: 0.0548 - acc: 0.9788 - val_loss: 1.9758 - val_acc: 0.7592\n",
            "Epoch 172/300\n",
            " - 2s - loss: 0.0550 - acc: 0.9770 - val_loss: 1.9735 - val_acc: 0.7586\n",
            "Epoch 173/300\n",
            " - 2s - loss: 0.0547 - acc: 0.9783 - val_loss: 2.0208 - val_acc: 0.7547\n",
            "Epoch 174/300\n",
            " - 2s - loss: 0.0560 - acc: 0.9781 - val_loss: 1.9059 - val_acc: 0.7586\n",
            "Epoch 175/300\n",
            " - 2s - loss: 0.0548 - acc: 0.9784 - val_loss: 1.7466 - val_acc: 0.7666\n",
            "Epoch 176/300\n",
            " - 2s - loss: 0.0540 - acc: 0.9775 - val_loss: 1.9342 - val_acc: 0.7586\n",
            "Epoch 177/300\n",
            " - 2s - loss: 0.0517 - acc: 0.9796 - val_loss: 2.0284 - val_acc: 0.7544\n",
            "Epoch 178/300\n",
            " - 2s - loss: 0.0527 - acc: 0.9795 - val_loss: 2.0550 - val_acc: 0.7554\n",
            "Epoch 179/300\n",
            " - 2s - loss: 0.0506 - acc: 0.9807 - val_loss: 2.0127 - val_acc: 0.7547\n",
            "Epoch 180/300\n",
            " - 2s - loss: 0.0508 - acc: 0.9802 - val_loss: 2.0299 - val_acc: 0.7576\n",
            "Epoch 181/300\n",
            " - 2s - loss: 0.0516 - acc: 0.9790 - val_loss: 2.1495 - val_acc: 0.7560\n",
            "Epoch 182/300\n",
            " - 2s - loss: 0.0509 - acc: 0.9798 - val_loss: 2.1577 - val_acc: 0.7565\n",
            "Epoch 183/300\n",
            " - 2s - loss: 0.0502 - acc: 0.9804 - val_loss: 1.9499 - val_acc: 0.7597\n",
            "Epoch 184/300\n",
            " - 2s - loss: 0.0508 - acc: 0.9791 - val_loss: 2.0124 - val_acc: 0.7592\n",
            "Epoch 185/300\n",
            " - 2s - loss: 0.0500 - acc: 0.9798 - val_loss: 2.0716 - val_acc: 0.7576\n",
            "Epoch 186/300\n",
            " - 2s - loss: 0.0499 - acc: 0.9786 - val_loss: 2.1777 - val_acc: 0.7541\n",
            "Epoch 187/300\n",
            " - 2s - loss: 0.0492 - acc: 0.9801 - val_loss: 2.0102 - val_acc: 0.7565\n",
            "Epoch 188/300\n",
            " - 2s - loss: 0.0462 - acc: 0.9811 - val_loss: 2.1994 - val_acc: 0.7539\n",
            "Epoch 189/300\n",
            " - 2s - loss: 0.0478 - acc: 0.9785 - val_loss: 2.1487 - val_acc: 0.7592\n",
            "Epoch 190/300\n",
            " - 2s - loss: 0.0493 - acc: 0.9790 - val_loss: 2.0803 - val_acc: 0.7602\n",
            "Epoch 191/300\n",
            " - 2s - loss: 0.0521 - acc: 0.9783 - val_loss: 2.1230 - val_acc: 0.7560\n",
            "Epoch 192/300\n",
            " - 2s - loss: 0.0460 - acc: 0.9809 - val_loss: 2.1855 - val_acc: 0.7549\n",
            "Epoch 193/300\n",
            " - 2s - loss: 0.0447 - acc: 0.9818 - val_loss: 2.1756 - val_acc: 0.7544\n",
            "Epoch 194/300\n",
            " - 2s - loss: 0.0471 - acc: 0.9802 - val_loss: 2.0584 - val_acc: 0.7533\n",
            "Epoch 195/300\n",
            " - 2s - loss: 0.0486 - acc: 0.9806 - val_loss: 2.1732 - val_acc: 0.7507\n",
            "Epoch 196/300\n",
            " - 2s - loss: 0.0487 - acc: 0.9800 - val_loss: 2.1580 - val_acc: 0.7544\n",
            "Epoch 197/300\n",
            " - 2s - loss: 0.0463 - acc: 0.9809 - val_loss: 2.1370 - val_acc: 0.7549\n",
            "Epoch 198/300\n",
            " - 2s - loss: 0.0480 - acc: 0.9804 - val_loss: 2.1239 - val_acc: 0.7554\n",
            "Epoch 199/300\n",
            " - 2s - loss: 0.0483 - acc: 0.9816 - val_loss: 2.1291 - val_acc: 0.7565\n",
            "Epoch 200/300\n",
            " - 2s - loss: 0.0449 - acc: 0.9815 - val_loss: 2.2518 - val_acc: 0.7533\n",
            "Epoch 201/300\n",
            " - 2s - loss: 0.0475 - acc: 0.9813 - val_loss: 2.2359 - val_acc: 0.7544\n",
            "Epoch 202/300\n",
            " - 2s - loss: 0.0453 - acc: 0.9818 - val_loss: 2.0990 - val_acc: 0.7597\n",
            "Epoch 203/300\n",
            " - 2s - loss: 0.0422 - acc: 0.9826 - val_loss: 2.2924 - val_acc: 0.7525\n",
            "Epoch 204/300\n",
            " - 2s - loss: 0.0484 - acc: 0.9802 - val_loss: 2.3564 - val_acc: 0.7533\n",
            "Epoch 205/300\n",
            " - 2s - loss: 0.0414 - acc: 0.9834 - val_loss: 2.3439 - val_acc: 0.7544\n",
            "Epoch 206/300\n",
            " - 2s - loss: 0.0444 - acc: 0.9808 - val_loss: 2.2488 - val_acc: 0.7533\n",
            "Epoch 207/300\n",
            " - 2s - loss: 0.0445 - acc: 0.9824 - val_loss: 2.2332 - val_acc: 0.7554\n",
            "Epoch 208/300\n",
            " - 2s - loss: 0.0443 - acc: 0.9814 - val_loss: 2.1090 - val_acc: 0.7560\n",
            "Epoch 209/300\n",
            " - 2s - loss: 0.0446 - acc: 0.9823 - val_loss: 2.3288 - val_acc: 0.7549\n",
            "Epoch 210/300\n",
            " - 2s - loss: 0.0430 - acc: 0.9827 - val_loss: 2.1405 - val_acc: 0.7554\n",
            "Epoch 211/300\n",
            " - 2s - loss: 0.0472 - acc: 0.9812 - val_loss: 2.0310 - val_acc: 0.7576\n",
            "Epoch 212/300\n",
            " - 2s - loss: 0.0426 - acc: 0.9813 - val_loss: 2.1989 - val_acc: 0.7560\n",
            "Epoch 213/300\n",
            " - 2s - loss: 0.0452 - acc: 0.9811 - val_loss: 2.2075 - val_acc: 0.7528\n",
            "Epoch 214/300\n",
            " - 2s - loss: 0.0391 - acc: 0.9848 - val_loss: 2.3009 - val_acc: 0.7528\n",
            "Epoch 215/300\n",
            " - 2s - loss: 0.0412 - acc: 0.9833 - val_loss: 2.3323 - val_acc: 0.7576\n",
            "Epoch 216/300\n",
            " - 2s - loss: 0.0402 - acc: 0.9843 - val_loss: 2.3828 - val_acc: 0.7560\n",
            "Epoch 217/300\n",
            " - 2s - loss: 0.0448 - acc: 0.9813 - val_loss: 2.2278 - val_acc: 0.7573\n",
            "Epoch 218/300\n",
            " - 2s - loss: 0.0430 - acc: 0.9819 - val_loss: 2.2545 - val_acc: 0.7528\n",
            "Epoch 219/300\n",
            " - 2s - loss: 0.0427 - acc: 0.9832 - val_loss: 2.2968 - val_acc: 0.7517\n",
            "Epoch 220/300\n",
            " - 2s - loss: 0.0418 - acc: 0.9840 - val_loss: 2.2088 - val_acc: 0.7557\n",
            "Epoch 221/300\n",
            " - 2s - loss: 0.0403 - acc: 0.9835 - val_loss: 2.3335 - val_acc: 0.7560\n",
            "Epoch 222/300\n",
            " - 2s - loss: 0.0421 - acc: 0.9825 - val_loss: 2.3006 - val_acc: 0.7523\n",
            "Epoch 223/300\n",
            " - 2s - loss: 0.0434 - acc: 0.9827 - val_loss: 2.3069 - val_acc: 0.7533\n",
            "Epoch 224/300\n",
            " - 2s - loss: 0.0428 - acc: 0.9831 - val_loss: 2.1536 - val_acc: 0.7549\n",
            "Epoch 225/300\n",
            " - 2s - loss: 0.0415 - acc: 0.9831 - val_loss: 2.3154 - val_acc: 0.7517\n",
            "Epoch 226/300\n",
            " - 2s - loss: 0.0396 - acc: 0.9841 - val_loss: 2.2656 - val_acc: 0.7570\n",
            "Epoch 227/300\n",
            " - 2s - loss: 0.0427 - acc: 0.9810 - val_loss: 2.3291 - val_acc: 0.7539\n",
            "Epoch 228/300\n",
            " - 2s - loss: 0.0453 - acc: 0.9827 - val_loss: 2.2999 - val_acc: 0.7491\n",
            "Epoch 229/300\n",
            " - 2s - loss: 0.0411 - acc: 0.9836 - val_loss: 2.2643 - val_acc: 0.7528\n",
            "Epoch 230/300\n",
            " - 2s - loss: 0.0389 - acc: 0.9840 - val_loss: 2.4343 - val_acc: 0.7541\n",
            "Epoch 231/300\n",
            " - 2s - loss: 0.0399 - acc: 0.9836 - val_loss: 2.5446 - val_acc: 0.7512\n",
            "Epoch 232/300\n",
            " - 2s - loss: 0.0388 - acc: 0.9842 - val_loss: 2.3751 - val_acc: 0.7533\n",
            "Epoch 233/300\n",
            " - 2s - loss: 0.0372 - acc: 0.9843 - val_loss: 2.5151 - val_acc: 0.7528\n",
            "Epoch 234/300\n",
            " - 2s - loss: 0.0390 - acc: 0.9836 - val_loss: 2.3520 - val_acc: 0.7541\n",
            "Epoch 235/300\n",
            " - 2s - loss: 0.0389 - acc: 0.9840 - val_loss: 2.3267 - val_acc: 0.7491\n",
            "Epoch 236/300\n",
            " - 2s - loss: 0.0393 - acc: 0.9832 - val_loss: 2.4146 - val_acc: 0.7512\n",
            "Epoch 237/300\n",
            " - 2s - loss: 0.0365 - acc: 0.9859 - val_loss: 2.3764 - val_acc: 0.7528\n",
            "Epoch 238/300\n",
            " - 2s - loss: 0.0395 - acc: 0.9831 - val_loss: 2.3684 - val_acc: 0.7544\n",
            "Epoch 239/300\n",
            " - 2s - loss: 0.0389 - acc: 0.9838 - val_loss: 2.4283 - val_acc: 0.7544\n",
            "Epoch 240/300\n",
            " - 2s - loss: 0.0381 - acc: 0.9846 - val_loss: 2.4111 - val_acc: 0.7554\n",
            "Epoch 241/300\n",
            " - 2s - loss: 0.0403 - acc: 0.9836 - val_loss: 2.3598 - val_acc: 0.7517\n",
            "Epoch 242/300\n",
            " - 2s - loss: 0.0371 - acc: 0.9847 - val_loss: 2.4077 - val_acc: 0.7544\n",
            "Epoch 243/300\n",
            " - 2s - loss: 0.0369 - acc: 0.9851 - val_loss: 2.3891 - val_acc: 0.7557\n",
            "Epoch 244/300\n",
            " - 2s - loss: 0.0396 - acc: 0.9839 - val_loss: 2.4622 - val_acc: 0.7523\n",
            "Epoch 245/300\n",
            " - 2s - loss: 0.0367 - acc: 0.9851 - val_loss: 2.3225 - val_acc: 0.7549\n",
            "Epoch 246/300\n",
            " - 2s - loss: 0.0374 - acc: 0.9849 - val_loss: 2.4094 - val_acc: 0.7539\n",
            "Epoch 247/300\n",
            " - 2s - loss: 0.0369 - acc: 0.9838 - val_loss: 2.4856 - val_acc: 0.7528\n",
            "Epoch 248/300\n",
            " - 2s - loss: 0.0366 - acc: 0.9852 - val_loss: 2.5551 - val_acc: 0.7499\n",
            "Epoch 249/300\n",
            " - 2s - loss: 0.0371 - acc: 0.9844 - val_loss: 2.4405 - val_acc: 0.7515\n",
            "Epoch 250/300\n",
            " - 2s - loss: 0.0362 - acc: 0.9856 - val_loss: 2.4685 - val_acc: 0.7523\n",
            "Epoch 251/300\n",
            " - 2s - loss: 0.0373 - acc: 0.9846 - val_loss: 2.5871 - val_acc: 0.7512\n",
            "Epoch 252/300\n",
            " - 2s - loss: 0.0380 - acc: 0.9839 - val_loss: 2.5280 - val_acc: 0.7512\n",
            "Epoch 253/300\n",
            " - 2s - loss: 0.0394 - acc: 0.9838 - val_loss: 2.3389 - val_acc: 0.7565\n",
            "Epoch 254/300\n",
            " - 2s - loss: 0.0372 - acc: 0.9846 - val_loss: 2.3784 - val_acc: 0.7549\n",
            "Epoch 255/300\n",
            " - 2s - loss: 0.0362 - acc: 0.9848 - val_loss: 2.5494 - val_acc: 0.7549\n",
            "Epoch 256/300\n",
            " - 2s - loss: 0.0339 - acc: 0.9856 - val_loss: 2.4722 - val_acc: 0.7554\n",
            "Epoch 257/300\n",
            " - 2s - loss: 0.0349 - acc: 0.9854 - val_loss: 2.6865 - val_acc: 0.7507\n",
            "Epoch 258/300\n",
            " - 2s - loss: 0.0350 - acc: 0.9851 - val_loss: 2.4526 - val_acc: 0.7507\n",
            "Epoch 259/300\n",
            " - 2s - loss: 0.0337 - acc: 0.9862 - val_loss: 2.5658 - val_acc: 0.7517\n",
            "Epoch 260/300\n",
            " - 2s - loss: 0.0344 - acc: 0.9858 - val_loss: 2.5635 - val_acc: 0.7507\n",
            "Epoch 261/300\n",
            " - 2s - loss: 0.0362 - acc: 0.9847 - val_loss: 2.5190 - val_acc: 0.7496\n",
            "Epoch 262/300\n",
            " - 2s - loss: 0.0338 - acc: 0.9862 - val_loss: 2.5002 - val_acc: 0.7517\n",
            "Epoch 263/300\n",
            " - 2s - loss: 0.0342 - acc: 0.9864 - val_loss: 2.5224 - val_acc: 0.7517\n",
            "Epoch 264/300\n",
            " - 2s - loss: 0.0365 - acc: 0.9846 - val_loss: 2.3845 - val_acc: 0.7544\n",
            "Epoch 265/300\n",
            " - 2s - loss: 0.0326 - acc: 0.9865 - val_loss: 2.4034 - val_acc: 0.7539\n",
            "Epoch 266/300\n",
            " - 2s - loss: 0.0331 - acc: 0.9863 - val_loss: 2.5448 - val_acc: 0.7549\n",
            "Epoch 267/300\n",
            " - 2s - loss: 0.0350 - acc: 0.9853 - val_loss: 2.5401 - val_acc: 0.7520\n",
            "Epoch 268/300\n",
            " - 2s - loss: 0.0331 - acc: 0.9859 - val_loss: 2.4575 - val_acc: 0.7576\n",
            "Epoch 269/300\n",
            " - 2s - loss: 0.0334 - acc: 0.9855 - val_loss: 2.7105 - val_acc: 0.7528\n",
            "Epoch 270/300\n",
            " - 2s - loss: 0.0343 - acc: 0.9854 - val_loss: 2.5765 - val_acc: 0.7517\n",
            "Epoch 271/300\n",
            " - 2s - loss: 0.0327 - acc: 0.9864 - val_loss: 2.5644 - val_acc: 0.7544\n",
            "Epoch 272/300\n",
            " - 2s - loss: 0.0324 - acc: 0.9862 - val_loss: 2.6129 - val_acc: 0.7533\n",
            "Epoch 273/300\n",
            " - 2s - loss: 0.0322 - acc: 0.9857 - val_loss: 2.5529 - val_acc: 0.7528\n",
            "Epoch 274/300\n",
            " - 2s - loss: 0.0314 - acc: 0.9870 - val_loss: 2.6802 - val_acc: 0.7496\n",
            "Epoch 275/300\n",
            " - 2s - loss: 0.0302 - acc: 0.9879 - val_loss: 2.6006 - val_acc: 0.7544\n",
            "Epoch 276/300\n",
            " - 2s - loss: 0.0343 - acc: 0.9864 - val_loss: 2.6537 - val_acc: 0.7544\n",
            "Epoch 277/300\n",
            " - 2s - loss: 0.0306 - acc: 0.9879 - val_loss: 2.5036 - val_acc: 0.7528\n",
            "Epoch 278/300\n",
            " - 2s - loss: 0.0310 - acc: 0.9873 - val_loss: 2.6646 - val_acc: 0.7554\n",
            "Epoch 279/300\n",
            " - 2s - loss: 0.0308 - acc: 0.9877 - val_loss: 2.7777 - val_acc: 0.7525\n",
            "Epoch 280/300\n",
            " - 2s - loss: 0.0310 - acc: 0.9870 - val_loss: 2.5433 - val_acc: 0.7528\n",
            "Epoch 281/300\n",
            " - 2s - loss: 0.0309 - acc: 0.9879 - val_loss: 2.7280 - val_acc: 0.7533\n",
            "Epoch 282/300\n",
            " - 2s - loss: 0.0304 - acc: 0.9877 - val_loss: 2.6439 - val_acc: 0.7528\n",
            "Epoch 283/300\n",
            " - 2s - loss: 0.0317 - acc: 0.9868 - val_loss: 2.7621 - val_acc: 0.7533\n",
            "Epoch 284/300\n",
            " - 2s - loss: 0.0329 - acc: 0.9869 - val_loss: 2.5695 - val_acc: 0.7560\n",
            "Epoch 285/300\n",
            " - 2s - loss: 0.0334 - acc: 0.9867 - val_loss: 2.5753 - val_acc: 0.7523\n",
            "Epoch 286/300\n",
            " - 2s - loss: 0.0335 - acc: 0.9865 - val_loss: 2.6856 - val_acc: 0.7533\n",
            "Epoch 287/300\n",
            " - 2s - loss: 0.0312 - acc: 0.9872 - val_loss: 2.6810 - val_acc: 0.7523\n",
            "Epoch 288/300\n",
            " - 2s - loss: 0.0337 - acc: 0.9872 - val_loss: 2.7942 - val_acc: 0.7485\n",
            "Epoch 289/300\n",
            " - 2s - loss: 0.0307 - acc: 0.9875 - val_loss: 2.7046 - val_acc: 0.7539\n",
            "Epoch 290/300\n",
            " - 2s - loss: 0.0298 - acc: 0.9891 - val_loss: 2.6117 - val_acc: 0.7528\n",
            "Epoch 291/300\n",
            " - 2s - loss: 0.0326 - acc: 0.9874 - val_loss: 2.8927 - val_acc: 0.7451\n",
            "Epoch 292/300\n",
            " - 2s - loss: 0.0313 - acc: 0.9865 - val_loss: 2.5895 - val_acc: 0.7501\n",
            "Epoch 293/300\n",
            " - 2s - loss: 0.0295 - acc: 0.9885 - val_loss: 2.7189 - val_acc: 0.7533\n",
            "Epoch 294/300\n",
            " - 2s - loss: 0.0310 - acc: 0.9866 - val_loss: 2.8766 - val_acc: 0.7523\n",
            "Epoch 295/300\n",
            " - 2s - loss: 0.0293 - acc: 0.9881 - val_loss: 2.6932 - val_acc: 0.7533\n",
            "Epoch 296/300\n",
            " - 2s - loss: 0.0308 - acc: 0.9881 - val_loss: 2.8745 - val_acc: 0.7459\n",
            "Epoch 297/300\n",
            " - 2s - loss: 0.0329 - acc: 0.9870 - val_loss: 2.7795 - val_acc: 0.7507\n",
            "Epoch 298/300\n",
            " - 2s - loss: 0.0300 - acc: 0.9886 - val_loss: 2.8636 - val_acc: 0.7480\n",
            "Epoch 299/300\n",
            " - 2s - loss: 0.0289 - acc: 0.9882 - val_loss: 2.8428 - val_acc: 0.7512\n",
            "Epoch 300/300\n",
            " - 2s - loss: 0.0286 - acc: 0.9892 - val_loss: 2.8847 - val_acc: 0.7523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al9enQ8qWnxc"
      },
      "source": [
        "# <8-1. AI 모델 훈련 결과 출력>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5WgRez9Wnxc",
        "outputId": "baf68fb9-cc0e-49e7-dc87-351f6be9ffb2"
      },
      "source": [
        "loss_and_metrics = model.evaluate(X_train,Y_train,batch_size=32)\n",
        "print(loss_and_metrics)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18806/18806 [==============================] - 2s 86us/step\n",
            "[0.2987128681107857, 0.9708603637137084]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVHBymt_Wnxc",
        "outputId": "0bfb90aa-68b8-4396-8c61-f6a2fb9922b2"
      },
      "source": [
        "loss_and_metrics2 = model.evaluate(X_test,Y_test,batch_size=32)\n",
        "print(loss_and_metrics2)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13242/13242 [==============================] - 1s 90us/step\n",
            "[0.8745124724121365, 0.915269596737653]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "D3NRYffjWnxd",
        "outputId": "68cdc315-ef3c-4056-be85-aded7b19a4e0"
      },
      "source": [
        "# 해당 코드 실행 시 val_acc 또는 acc에서 오류 발생 시, 각각 val_accuracy 와 accuracy로 바꿔서 실행하도록 한다.\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.plot(history.history['acc'])\n",
        "plt.title('Accuracy During Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Validation Accuracy','Training Accuracy'])\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f767af87850>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "EEJr3NvfWnxd",
        "outputId": "054cade1-684e-4111-e0a9-8305164a4e65"
      },
      "source": [
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Loss During Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Validation Loss','Training Loss'])\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqaXax7MWnxd"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}